{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34de416d",
   "metadata": {},
   "source": [
    "# Credit Card Fraud Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbf2c39",
   "metadata": {},
   "source": [
    "When we make any transaction while purchasing any product online — a good amount of people prefer credit cards. The credit limit in credit cards sometimes helps us me making purchases even if we don’t have the amount at that time. but, on the other hand, these features are misused by cyber attackers.\n",
    "\n",
    "To tackle this problem we need a system that can abort the transaction if it finds fishy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "94fb4180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install termcolor\n",
    "# !pip install numpy\n",
    "# !pip install scipy\n",
    "# !pip install pandas\n",
    "# !pip install seaborn\n",
    "# !pip install matplotlib\n",
    "# !pip install scikit-learn\n",
    "# !pip install statsmodels\n",
    "# !pip install xgboost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de6b030d",
   "metadata": {},
   "source": [
    "### #uncomment the above if you need to install that library "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7407827",
   "metadata": {},
   "source": [
    "# Importing Necessary Libraries\n",
    "\n",
    "It is a good practice to import all the necessary libraries in one place — so that we can modify them quickly.\n",
    "\n",
    "For this credit card data, the features that we have in the dataset are the transformed version of PCA, so we will not need to perform the feature selection again. Otherwise, it is recommended to use RFE, RFECV, SelectKBest and VIF score to find the best features for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8478c025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 432x216 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Packages related to general operating system & warnings\n",
    "import os \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "#Packages related to data importing, manipulation, exploratory data #analysis, data understanding\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas import Series, DataFrame\n",
    "from termcolor import colored as cl # text customization\n",
    "#Packages related to data visualizaiton\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "#Setting plot sizes and type of plot\n",
    "plt.rc(\"font\", size=14)\n",
    "plt.rcParams['axes.grid'] = True\n",
    "plt.figure(figsize=(6,3))\n",
    "plt.gray()\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.impute import MissingIndicator, SimpleImputer\n",
    "from sklearn.preprocessing import  PolynomialFeatures, KBinsDiscretizer, FunctionTransformer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, MaxAbsScaler\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, LabelBinarizer, OrdinalEncoder\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.tsa as tsa\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, ElasticNet, Lasso, Ridge\n",
    "from sklearn.neighbors import KNeighborsClassifier, KNeighborsRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, export_graphviz, export_text\n",
    "from sklearn.ensemble import BaggingClassifier, BaggingRegressor,RandomForestClassifier,RandomForestRegressor\n",
    "from sklearn.ensemble import GradientBoostingClassifier,GradientBoostingRegressor, AdaBoostClassifier, AdaBoostRegressor \n",
    "from sklearn.svm import LinearSVC, LinearSVR, SVC, SVR\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e63501cd",
   "metadata": {},
   "source": [
    "# Importing Dataset\n",
    "\n",
    "Importing the dataset is pretty much simple. You can use pandas module in python to import it.\n",
    "\n",
    "Run the below command to import your data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "41eac391",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"creditcard.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a05f96",
   "metadata": {},
   "source": [
    "# Data Processing & Understanding\n",
    "\n",
    "The one main thing you will notice about this data is that — the dataset is imbalanced towards a feature. Which seems pretty valid for such kind of data. Because today many banks have adopted different security mechanisms — so it is harder for hackers to make such moves.\n",
    "\n",
    "Still, sometimes when there is some vulnerability in the system — the chance of such activities can increase.\n",
    "\n",
    "That’s why we can see the majority of transactions belongs to our datasets are normal and only a few percentages of transactions are fraudulent.\n",
    "\n",
    "Let’s check the transaction distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96a23ba4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mTotal number of Trnsactions are 284807\u001b[0m\n",
      "\u001b[1mNumber of Normal Transactions are 284315\u001b[0m\n",
      "\u001b[1mNumber of fraudulent Transactions are 492\u001b[0m\n",
      "\u001b[1mPercentage of fraud Transactions is 0.17\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "Total_transactions = len(data)\n",
    "normal = len(data[data.Class == 0])\n",
    "fraudulent = len(data[data.Class == 1])\n",
    "fraud_percentage = round(fraudulent/normal*100, 2)\n",
    "print(cl('Total number of Trnsactions are {}'.format(Total_transactions), attrs = ['bold']))\n",
    "print(cl('Number of Normal Transactions are {}'.format(normal), attrs = ['bold']))\n",
    "print(cl('Number of fraudulent Transactions are {}'.format(fraudulent), attrs = ['bold']))\n",
    "print(cl('Percentage of fraud Transactions is {}'.format(fraud_percentage), attrs = ['bold']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06eff489",
   "metadata": {},
   "source": [
    "Only 0.17% of transactions are fraudulent.\n",
    "\n",
    "We can also check for null values using the following line of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "983c11e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 284807 entries, 0 to 284806\n",
      "Data columns (total 31 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   Time    284807 non-null  float64\n",
      " 1   V1      284807 non-null  float64\n",
      " 2   V2      284807 non-null  float64\n",
      " 3   V3      284807 non-null  float64\n",
      " 4   V4      284807 non-null  float64\n",
      " 5   V5      284807 non-null  float64\n",
      " 6   V6      284807 non-null  float64\n",
      " 7   V7      284807 non-null  float64\n",
      " 8   V8      284807 non-null  float64\n",
      " 9   V9      284807 non-null  float64\n",
      " 10  V10     284807 non-null  float64\n",
      " 11  V11     284807 non-null  float64\n",
      " 12  V12     284807 non-null  float64\n",
      " 13  V13     284807 non-null  float64\n",
      " 14  V14     284807 non-null  float64\n",
      " 15  V15     284807 non-null  float64\n",
      " 16  V16     284807 non-null  float64\n",
      " 17  V17     284807 non-null  float64\n",
      " 18  V18     284807 non-null  float64\n",
      " 19  V19     284807 non-null  float64\n",
      " 20  V20     284807 non-null  float64\n",
      " 21  V21     284807 non-null  float64\n",
      " 22  V22     284807 non-null  float64\n",
      " 23  V23     284807 non-null  float64\n",
      " 24  V24     284807 non-null  float64\n",
      " 25  V25     284807 non-null  float64\n",
      " 26  V26     284807 non-null  float64\n",
      " 27  V27     284807 non-null  float64\n",
      " 28  V28     284807 non-null  float64\n",
      " 29  Amount  284807 non-null  float64\n",
      " 30  Class   284807 non-null  int64  \n",
      "dtypes: float64(30), int64(1)\n",
      "memory usage: 67.4 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da993339",
   "metadata": {},
   "source": [
    "As per the count per column, we have no null values. Also, feature selection is not the case for this use case. Anyway, you can try applying feature selection mechanisms to check if the results are optimised.\n",
    "\n",
    "I have observed in our data 28 features are transformed versions of PCA but the Amount is the original one. And, while checking the minimum and maximum is in the amount — I found the difference is huge that can deviate our result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5784afe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.0, 25691.16)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(data.Amount), max(data.Amount)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e9c074",
   "metadata": {},
   "source": [
    "In this case, it is a good practice to scale this variable. We can use a standard scaler to make it fix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "61e39919",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "amount = data['Amount'].values\n",
    "data['Amount'] = sc.fit_transform(amount.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2d6855c",
   "metadata": {},
   "source": [
    "We have one more variable which is the time which can be an external deciding factor — but in our modelling process, we can drop it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fdb28a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['Time'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ebf990",
   "metadata": {},
   "source": [
    "We can also check for any duplicate transactions. Before removing any duplicate transaction, we are having 284807 transactions in our data. Let’s remove the duplicate and observe the changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a2470817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(284807, 30)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20016784",
   "metadata": {},
   "source": [
    "Run the below line of code to remove any duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ddedd1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bcfc50e",
   "metadata": {},
   "source": [
    "Let’s now check the count again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3dfd216b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(275663, 30)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af192fca",
   "metadata": {},
   "source": [
    "So, we were having around ~9000 duplicate transactions.\n",
    "\n",
    "Here we go!! We now have properly scaled data with no duplicate, no missing. Let’s now split it for our model building."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399c12b0",
   "metadata": {},
   "source": [
    "# Train & Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d0e66a",
   "metadata": {},
   "source": [
    "Before splitting train & test — we need to define dependent and independent variables. The dependent variable is also known as X and the independent variable is known as y."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f1de8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop('Class', axis = 1).values\n",
    "y = data['Class'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a853ea",
   "metadata": {},
   "source": [
    "Now, let split our train and test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bb294831",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34cc623",
   "metadata": {},
   "source": [
    "That’s it. We now have two different data set — Train data we will be used for training our model and the data which is unseen will be used for testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d3b1b6",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a9f81a9",
   "metadata": {},
   "source": [
    "We will be trying different machine learning models one by one. Defining models are much easier. A single line of code can define our model. And, in the same way, a single line of code can fit the model on our data.\n",
    "\n",
    "We can also tune these models by selecting different optimized parameters. But, if the accuracy is better even with less parameter tuning then — no need to make it complex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b25f28",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cced0445",
   "metadata": {},
   "outputs": [],
   "source": [
    "DT = DecisionTreeClassifier(max_depth = 4, criterion = 'entropy')\n",
    "DT.fit(X_train, y_train)\n",
    "dt_yhat = DT.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9be6ae2b",
   "metadata": {},
   "source": [
    "Let’s check the accuracy of our decision tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3159a50d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of the Decision Tree model is 0.9991583957281328\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy score of the Decision Tree model is {}'.format(accuracy_score(y_test, dt_yhat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd51473",
   "metadata": {},
   "source": [
    "Checking F1-Score for the decision tree model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f96ccee5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of the Decision Tree model is 0.7521367521367521\n"
     ]
    }
   ],
   "source": [
    "print('F1 score of the Decision Tree model is {}'.format(f1_score(y_test, dt_yhat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5c91af",
   "metadata": {},
   "source": [
    "Checking the confusion matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92a1a39d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[68770,    18],\n",
       "       [   40,    88]], dtype=int64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(y_test, dt_yhat, labels = [0, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d8c31e",
   "metadata": {},
   "source": [
    "Here, the first row represents positive and the second row represents negative. So, we have 68782 as true positive and 18 are false positive. That says, out of 68782+18=68800, we have 68782 that are successfully classified as a normal transaction and 18 were falsely classified as normal — but they were fraudulent.\n",
    "\n",
    "Let’s now try different models and check their performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e38d4b1",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cd9a61ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 7\n",
    "KNN = KNeighborsClassifier(n_neighbors = n)\n",
    "KNN.fit(X_train, y_train)\n",
    "knn_yhat = KNN.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc8a82e",
   "metadata": {},
   "source": [
    "Let’s check the accuracy of our K-Nearest Neighbors model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "9f351a93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of the K-Nearest Neighbors model is 0.999288989494457\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy score of the K-Nearest Neighbors model is {}'.format(accuracy_score(y_test, knn_yhat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb5a03f",
   "metadata": {},
   "source": [
    "Checking F1-Score for the K-Nearest Neighbors model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d53a308c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of the K-Nearest Neighbors model is 0.7949790794979079\n"
     ]
    }
   ],
   "source": [
    "print('F1 score of the K-Nearest Neighbors model is {}'.format(f1_score(y_test, knn_yhat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14c2b33",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e87bd2ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "lr_yhat = lr.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359b082e",
   "metadata": {},
   "source": [
    "Let’s check the accuracy of our Logistic Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58f37c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of the Logistic Regression model is 0.9989552498694062\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy score of the Logistic Regression model is {}'.format(accuracy_score(y_test, lr_yhat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc306e3",
   "metadata": {},
   "source": [
    "Checking F1-Score for the Logistic Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a954e15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of the Logistic Regression model is 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "print('F1 score of the Logistic Regression model is {}'.format(f1_score(y_test, lr_yhat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5738a92",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "91e5a1b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(max_depth = 4)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_yhat = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fbf0473",
   "metadata": {},
   "source": [
    "Let’s check the accuracy of our Random Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92136a55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of the Random Forest model is 0.9991583957281328\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy score of the Random Forest model is {}'.format(accuracy_score(y_test, rf_yhat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e46210",
   "metadata": {},
   "source": [
    "Checking F1-Score for the Random Forest model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e78a07bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of the Random Forest model is 0.7339449541284404\n"
     ]
    }
   ],
   "source": [
    "print('F1 score of the Random Forest model is {}'.format(f1_score(y_test, rf_yhat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c6587f",
   "metadata": {},
   "source": [
    "## XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0704ebe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb = XGBClassifier(max_depth = 4)\n",
    "xgb.fit(X_train, y_train)\n",
    "xgb_yhat = xgb.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8af281a",
   "metadata": {},
   "source": [
    "Let’s check the accuracy of our XGBoost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "99ae1a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy score of the XGBoost model is 0.999506645771664\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy score of the XGBoost model is {}'.format(accuracy_score(y_test, xgb_yhat)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "453c8b31",
   "metadata": {},
   "source": [
    "Checking F1-Score for the XGBoost model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f5cdf250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score of the XGBoost model is 0.8495575221238937\n"
     ]
    }
   ],
   "source": [
    "print('F1 score of the XGBoost model is {}'.format(f1_score(y_test, xgb_yhat)))"
   ]
  },
  
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac52dd0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
